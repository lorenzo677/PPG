\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@refcontext{none/global//global/global}
\abx@aux@cite{MBA}
\abx@aux@segm{0}{0}{MBA}
\abx@aux@cite{miniforge-doc}
\abx@aux@segm{0}{0}{miniforge-doc}
\abx@aux@cite{tensorflow2015-whitepaper}
\abx@aux@segm{0}{0}{tensorflow2015-whitepaper}
\abx@aux@cite{Matsumura2015}
\abx@aux@segm{0}{0}{Matsumura2015}
\abx@aux@cite{vae_paper}
\abx@aux@segm{0}{0}{vae_paper}
\babel@aux{english}{}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Photoplethysmography}{1}{subsection.1.1}\protected@file@percent }
\newlabel{ppg_intro}{{1.1}{1}{Photoplethysmography}{subsection.1.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of a photoplethysmogram present in the database. The figure shows only the first part of the plot, that is much larger.}}{1}{figure.1}\protected@file@percent }
\newlabel{fig:ppgexample}{{1}{1}{Example of a photoplethysmogram present in the database. The figure shows only the first part of the plot, that is much larger}{figure.1}{}}
\abx@aux@cite{vae_scheme}
\abx@aux@segm{0}{0}{vae_scheme}
\abx@aux@cite{kernel_cov}
\abx@aux@segm{0}{0}{kernel_cov}
\abx@aux@cite{8308186}
\abx@aux@segm{0}{0}{8308186}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}$\beta -$Variational Autoencoder}{2}{subsection.1.2}\protected@file@percent }
\newlabel{vae:intro}{{1.2}{2}{$\beta -$Variational Autoencoder}{subsection.1.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Structure of a VAE. It is possible to distinguish the \emph  {encoder} that, in the particular case of a \emph  {Convolutional}$-$ VAE, performs convolution operations, the \emph  { latent space}, in which the important information are contained, and the \emph  {decoder}, that tries to reconstruct the output starting from the latent space and provide us the Neural Network output.}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:vaebasic}{{2}{2}{Structure of a VAE. It is possible to distinguish the \emph {encoder} that, in the particular case of a \emph {Convolutional}$-$ VAE, performs convolution operations, the \emph { latent space}, in which the important information are contained, and the \emph {decoder}, that tries to reconstruct the output starting from the latent space and provide us the Neural Network output}{figure.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Convolutions}{2}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{convolution}{{1.2.1}{2}{Convolutions}{subsubsection.1.2.1}{}}
\abx@aux@cite{rep_trick}
\abx@aux@segm{0}{0}{rep_trick}
\abx@aux@cite{9244048}
\abx@aux@segm{0}{0}{9244048}
\abx@aux@cite{9244048}
\abx@aux@segm{0}{0}{9244048}
\abx@aux@cite{KL}
\abx@aux@segm{0}{0}{KL}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of how a \emph  {kernel} (or \emph  {filter}) matrix convolution works. The source matrix is convoluted with the \emph  {kernel} matrix. This leads to a number that becomes the output value of a pixel.}}{3}{figure.3}\protected@file@percent }
\newlabel{fig:kernel}{{3}{3}{Example of how a \emph {kernel} (or \emph {filter}) matrix convolution works. The source matrix is convoluted with the \emph {kernel} matrix. This leads to a number that becomes the output value of a pixel}{figure.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Reparametrization Trick}{3}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Loss Function}{3}{subsubsection.1.2.3}\protected@file@percent }
\abx@aux@cite{database}
\abx@aux@segm{0}{0}{database}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces In this Figure it is possible to appreciate the differences in the two approaches for the sampling of the latent distributions. On the left side we have the original approach, where the stochasticity is present in the $Z$ node. This prevent the backpropagation and the learning of the NN. On the left side, the stochastic part is moved through the parameter $\varepsilon $, allowing the backpropagation through the node $Z$.}}{4}{figure.4}\protected@file@percent }
\newlabel{fig:rep_trick}{{4}{4}{In this Figure it is possible to appreciate the differences in the two approaches for the sampling of the latent distributions. On the left side we have the original approach, where the stochasticity is present in the $Z$ node. This prevent the backpropagation and the learning of the NN. On the left side, the stochastic part is moved through the parameter $\varepsilon $, allowing the backpropagation through the node $Z$}{figure.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The database}{4}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of the ages between the patients. In green there are the entries used to train the NN, while in red the entries excluded from this process. }}{4}{figure.5}\protected@file@percent }
\newlabel{fig:agehistogram}{{5}{4}{Distribution of the ages between the patients. In green there are the entries used to train the NN, while in red the entries excluded from this process}{figure.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Preprocessing}{4}{subsection.2.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Here are reported all the best parameters used to train the NN.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:parameters}{{1}{5}{Here are reported all the best parameters used to train the NN}{table.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The Network}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{6}{section.3}\protected@file@percent }
\newlabel{Results}{{3}{6}{Results}{section.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Loss function calculated for the train dataset (blue) and the validation one (red). The black dashed line correspond at the epoch at which the $\beta $ parameter changed from 0 to $0.2$.}}{6}{figure.7}\protected@file@percent }
\newlabel{fig:loss}{{7}{6}{Loss function calculated for the train dataset (blue) and the validation one (red). The black dashed line correspond at the epoch at which the $\beta $ parameter changed from 0 to $0.2$}{figure.7}{}}
\abx@aux@cite{cardiac_cycle}
\abx@aux@segm{0}{0}{cardiac_cycle}
\abx@aux@cite{ppg_cycle}
\abx@aux@segm{0}{0}{ppg_cycle}
\abx@aux@cite{dicrotic-notch}
\abx@aux@segm{0}{0}{dicrotic-notch}
\abx@aux@cite{review}
\abx@aux@segm{0}{0}{review}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Here it is possible to see the latent space. On the left there are only the points of the young people, on the right only the ones coming from the older ones and in the middle the points come from both the categories.}}{7}{figure.6}\protected@file@percent }
\newlabel{fig:youngold}{{6}{7}{Here it is possible to see the latent space. On the left there are only the points of the young people, on the right only the ones coming from the older ones and in the middle the points come from both the categories}{figure.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusions}{7}{section.4}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces In this figure it is possible to observe the graph of a peak of the heartbeat recorded with a PPG technique. It is possible to identify the \emph  {systolic peak} (the higher peak), the \emph  {diastolic peak} and the \emph  {dicrotic notch}, where there is a relative minimum.}}{7}{figure.11}\protected@file@percent }
\newlabel{fig:ppgcycle}{{11}{7}{In this figure it is possible to observe the graph of a peak of the heartbeat recorded with a PPG technique. It is possible to identify the \emph {systolic peak} (the higher peak), the \emph {diastolic peak} and the \emph {dicrotic notch}, where there is a relative minimum}{figure.11}{}}
\abx@aux@cite{review}
\abx@aux@segm{0}{0}{review}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Latent space with the points colored according to the age of the people.}}{8}{figure.8}\protected@file@percent }
\newlabel{fig:clusters}{{8}{8}{Latent space with the points colored according to the age of the people}{figure.8}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Latent space and, in black, the points give to the decoder in order to obtain the plots present in Figure \ref  {fig:latent_space}.}}{8}{figure.9}\protected@file@percent }
\newlabel{fig:lastbest2dbeta0}{{9}{8}{Latent space and, in black, the points give to the decoder in order to obtain the plots present in Figure \ref {fig:latent_space}}{figure.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces In this Figure are shown the peaks that have been generated by sampling the latent space in points distributed on a grid between $-1.7$ and $1$ in the \emph  {x-axis} and $-1$ and $1$ along the \emph  {y-axis}. Between the brackets there are the coordinates of the sampled points in the latent space. The top-left part of this grid should not be considered. In the bottom-left we can find the generated PPG near, in the latent space, to young people's PPGs. In the bottom-left part, there is a region in which we have PPG near to both young and old people, while in the top-right part, there is a region near to the old people's PPG.}}{9}{figure.10}\protected@file@percent }
\newlabel{fig:latent_space}{{10}{9}{In this Figure are shown the peaks that have been generated by sampling the latent space in points distributed on a grid between $-1.7$ and $1$ in the \emph {x-axis} and $-1$ and $1$ along the \emph {y-axis}. Between the brackets there are the coordinates of the sampled points in the latent space. The top-left part of this grid should not be considered. In the bottom-left we can find the generated PPG near, in the latent space, to young people's PPGs. In the bottom-left part, there is a region in which we have PPG near to both young and old people, while in the top-right part, there is a region near to the old people's PPG}{figure.10}{}}
\abx@aux@read@bbl@mdfivesum{7AAA0A58E342537EA3901B483C65DCBC}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{MBA}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{miniforge-doc}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{tensorflow2015-whitepaper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Matsumura2015}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{vae_paper}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{vae_scheme}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kernel_cov}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{8308186}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rep_trick}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{9244048}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{KL}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{database}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{cardiac_cycle}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ppg_cycle}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{dicrotic-notch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{review}{none/global//global/global}
\gdef \@abspage@last{10}
